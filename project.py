# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MudZWjfDoPMj0FRAefwEml3MdoBpLxbn
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Import Libraries**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""# **Raw & Initial Data Exploration**


*   Exploratory Data Analysis
*   Examine rows, missing values, distributions and skewnesss


"""

original_data = pd.read_csv('/content/drive/MyDrive/Fraud.csv')

origData = pd.DataFrame(original_data)

# presence of categorical variables - eventually will transform to numeric
print(origData.info(), "\n")

# check for missing values - this data set has no missing values
origData.isna().sum()

# search for duplicate rows - this dataset has no duplicate rows
origData[origData.duplicated()].sum()

# some customers have more than one transaction
 origData.duplicated('nameOrig').sum()

# multiple transactions are have the same recipient
 origData.duplicated('nameDest').sum()

# values have wide range variation - may need to normalize
origData.describe()

# Kaggle description indicates only customers start a transaction, but there are both merchants and customers who can receive transcation
# Let's check that this is true

print(origData.sort_values('nameOrig', ascending=False)['nameOrig'].head) # sorting in opposite alphabetical order
print('------------------------------')
print(origData.sort_values('nameDest', ascending=False)['nameDest'].head)
# this is true: transcations only made by customers and receipts are both merchants and customers

# proportion of fraudulent : non-fraudulent transactions -
# 99.87 % are non-fraudulent
# 0.129 % are fraudulent
origData['isFraud'].value_counts() / origData.shape[0] * 100

# analyzing the # of transactions/day

def convert_step_to_hour_day(step):
    day = (step - 1) // 24 + 1
    hour = (step - 1) % 24
    return hour, day

origData['hour'], origData['day'] = zip(*origData['step'].apply(convert_step_to_hour_day))

origData = origData[['day','hour'] + origData.columns[:-2].tolist()] # moves day and hour to first and second column
origData = origData.sort_values(by = ['day', 'hour']).reset_index() # sort by day and hour of day
origData = origData.drop(['index', 'step'], axis = 1)

# uneven distribution number of transactions per day - could this affect the model?
origData['day'].value_counts(sort=False)

"""# **Adjusted Data Exploration**


*   Distribution of transactions are heavily skewed but will not affect our results
*   Due to the proportion of fraudulent to non-fraudulent transactions, our results could be heavily impacted and can lead to inaccurate results. Accuracy will be very high as it has very little room for error due to the proportion sizes of fraudulent and non-fraudulent.


"""

# even out the proportion of fraudulent to non-fraudulent in the sample

# since we have 8213 fraudulent transactions, we can select 10000 random samples from the non-fraudulent set
cardData = original_data[original_data['isFraud'] == 0].sample(n = 10000, random_state = 0)

# combine it with fraudulent transactions to make one data set
cardData = pd.concat([original_data[original_data['isFraud'] == 1], cardData], axis=0)

# new dimensions
print(cardData.shape)

# percentage of fradulent vs not fradulent
# 54.9% are non-fraudulent
# 45.09% are fraudulent
cardData['isFraud'].value_counts() / cardData.shape[0] * 100

# the # of transactions per day are also less sparse
cardData['day'].value_counts(sort=False)

# each customer is unique
cardData.duplicated('nameOrig').sum()

# some transactions headed to the same recipient
cardData.duplicated('nameDest').sum()

fraud_percentage = cardData['isFraud'].value_counts() / cardData.shape[0] * 100
plt.figure(figsize=(6, 4))
bars = plt.bar(fraud_percentage.index, fraud_percentage.values, color=['mediumslateblue', 'plum'])
plt.title('Distribution of Fraudulent Transactions')
plt.ylabel('Percentage')
plt.xticks(fraud_percentage.index, ['Non-Fraudulent', 'Fraudulent'])
plt.ylim(0, 100)
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height / 2, f'{height:.1f}%', ha='center', va='center')
plt.show()

# sns.countplot(x = 'type', data = data_set)

ax = sns.countplot(x = 'type', data = cardData)
plt.title('Transactions by Type')
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()),
                ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')
plt.show()
# most transaction types are payments and cash outs

# make a copy for cleansing
dataSet = cardData.copy()

"""# **Data Cleansing**


*   Handling missing values --> We have none.
*   Dealing with duplicated rows --> We have none.
*   Converting step into hour and day (different time breakdown)
*   Encoding categorical variables --> We will hot-encode the type and merchant/customer original and destination columns to make numeric.
*   Convert the id of the Original transcation into numeric and id of the Destination transcation into numeric.
"""

dataSet

# One-hot encoding for type
categorical_ = ["type"]
dataSet = pd.get_dummies(dataSet, columns = categorical_)

# One-hot encoding for transcation recipient type
cate = ["recipient"]
dataSet['recipient'] = dataSet['nameDest'].str[0]
dataSet = pd.get_dummies(dataSet, columns = cate)

dataSet

plt.figure(figsize=(10, 5))
ax = sns.histplot(data = dataSet[dataSet['isFraud'] == 1], color = 'blue', x = 'hour', bins = 23, kde = True, edgecolor = 'black', alpha = 0.6)
plt.title('Distribution of Fraud Transactions in 24 Hours', fontsize = 14, fontweight = 'bold')
plt.xlabel('Hour of the Day', fontsize = 12)
plt.ylabel('Count', fontsize = 12)
plt.xlim(0, 23)
plt.xticks(fontsize = 10)
plt.yticks(fontsize = 10)
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.0f'),
                (p.get_x() + p.get_width() / 2, p.get_height()),
                ha = 'center', va = 'bottom', fontsize = 10)
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(data = origData[origData['isFraud'] == 1], x = 'type', palette = 'Set2',order = origData['type'].unique())
plt.title('Total Number of Fraud Transactions by Transaction Type', fontsize=14, fontweight='bold')
plt.xlabel('Transaction Type', fontsize=12)
plt.ylabel('Total Number of Transactions', fontsize=12)
plt.xticks(rotation = 45, ha = 'right')
plt.yticks(fontsize = 10)
for p in plt.gca().patches:
    plt.gca().annotate(format(p.get_height(), '.0f'),
                       (p.get_x() + p.get_width() / 2, p.get_height()),
                       ha = 'center', va = 'bottom', fontsize = 10)
plt.tight_layout()
plt.show()

# remove 'C' so that value is completely numeric
# all are customers so no concern for the starting letter
dataSet['nameOrig']= dataSet['nameOrig'].str[1:]
# no need to one-hot encode since only customers make transcations

# used so that the number of unique values stay the same and prevent merging id for merchant and customer
def uniqueCodes(value):
    if value[0] == 'C':
        return '0' + value[1:]
    elif value[0] == 'M':
        return '1' + value[1:]
    return value

# use function on nameDest since both customers and merchants are recipients
# keep the number of unique values in this column same as before we remove prefixes
dataSet['nameDest'] = dataSet['nameDest'].apply(uniqueCodes)

"""# **Feature Extraction**
- Maintain relevant features we think will help us predict fraudulent transcations
- Convert all the transcation and balance values to create relationship of transcation amount and balance. See if there is an issue with it and should be flagged.
- We will not be using PCA since PCA applies to continous numerical variables, and majority of our numerical variables have been encoded numerically.
- Will look at correlation analysis (correlation matrix) to see what features cause top correlation
"""

# % of the transaction amount for each balance variable
dataSet['Amount/OldBalanceOrg'] = dataSet['amount'] / dataSet['oldbalanceOrg']
dataSet['Amount/NewBalanceOrg'] = dataSet['amount'] / dataSet['newbalanceOrig']
dataSet['Amount/OldBalanceDest'] = dataSet['amount'] / dataSet['oldbalanceDest']
dataSet['Amount/NewBalanceDest'] = dataSet['amount'] / dataSet['newbalanceDest']

# if transaction amount is greater than balance, then % > 1 but we need to keep it in between 0 and 1
# account for division by 0 by setting column to 1 if balnce is 0
def transactionRatio(df, amount, balance, colname):
    df[colname] = df[amount] / df[balance]
    df[colname] = np.where((df[amount] / df[balance] > 1) | (df[balance] <= 0), 1, df[colname])
    return df

dataSet = transactionRatio(dataSet, 'amount', 'oldbalanceOrg', 'Amount/OldBalanceOrg')
dataSet = transactionRatio(dataSet, 'amount', 'newbalanceOrig', 'Amount/NewBalanceOrg')
dataSet = transactionRatio(dataSet, 'amount', 'oldbalanceDest', 'Amount/OldBalanceDest')
dataSet = transactionRatio(dataSet, 'amount', 'newbalanceDest', 'Amount/NewBalanceDest')

# flag for whether the all the account balances are empty or not
def balanceFlag(df, balance, newcol):
    df[newcol] = np.where(df[balance] > 0, 1, 0)
    return df

dataSet = balanceFlag(dataSet, 'oldbalanceOrg', 'oldBalOrig Flag')
dataSet = balanceFlag(dataSet, 'newbalanceOrig', 'newBalOrig Flag')
dataSet = balanceFlag(dataSet, 'oldbalanceDest', 'oldBalDest Flag')
dataSet = balanceFlag(dataSet, 'newbalanceDest', 'newBalDest Flag')

data_clean = dataSet.drop(['oldbalanceOrg', 'isFlaggedFraud', 'oldbalanceDest', 'day', 'hour', 'Amount/NewBalanceOrg', 'Amount/NewBalanceDest','newbalanceOrig',
                         'newbalanceDest', 'Amount/OldBalanceOrg', 'Amount/OldBalanceDest'], axis=1)
data_clean

correlation_matrix = data_clean.corr()
target = correlation_matrix['isFraud']
print(target)

# Plot the heatmap
plt.figure(figsize=(10,5))
sns.heatmap(correlation_matrix, annot=True)

isfraud = correlation_matrix.columns.get_loc('isFraud')
correlation_matrix.columns[correlation_matrix.iloc[:, isfraud] > 0].tolist()

"""We noticed that there were a few variables in the isFraud row that had negative correlation values. We wanted to explore these more."""

# run to check how many frauds were done where a recipient was a merchant (there should be 0)
print((data_clean[(data_clean['recipient_M'] == 1) & (data_clean['isFraud'] == 1)]).shape[0])
# no frauds were commited with recipient merchant accounts, only customers

# Now we want to see how this applies to all the other columns
categories = ['Recipient Merchant', 'Debit', 'Payment', 'Cash In', 'Old Balance Orig', 'New Balance Orig',
              'Old Balance Dest', 'New Balance Dest', 'Transfer', 'Cash Out', 'Recipient Customer']
counts = [(data_clean[(data_clean['recipient_M'] == 1) & (data_clean['isFraud'] == 1)]).shape[0],
          (data_clean[(data_clean['type_DEBIT'] == 1) & (data_clean['isFraud'] == 1)]).shape[0],
          (data_clean[(data_clean['type_PAYMENT'] == 1) & (data_clean['isFraud'] == 1)]).shape[0],
          (data_clean[(data_clean['type_CASH_IN'] == 1) & (data_clean['isFraud'] == 1)]).shape[0],
          (data_clean[(data_clean['oldBalOrig Flag'] == 1) & (data_clean['isFraud'] == 1)]).shape[0],
          (data_clean[(data_clean['newBalOrig Flag'] == 1) & (data_clean['isFraud'] == 1)]).shape[0],
          (data_clean[(data_clean['oldBalDest Flag'] == 1) & (data_clean['isFraud'] == 1)]).shape[0],
          (data_clean[(data_clean['newBalDest Flag'] == 1) & (data_clean['isFraud'] == 1)]).shape[0],
          (data_clean[(data_clean['type_TRANSFER'] == 1) & (data_clean['isFraud'] == 1)]).shape[0],
          (data_clean[(data_clean['type_CASH_OUT'] == 1) & (data_clean['isFraud'] == 1)]).shape[0],
          (data_clean[(data_clean['recipient_C'] == 1) & (data_clean['isFraud'] == 1)]).shape[0]]

plt.figure(figsize=(12, 6))
sns.barplot(x=categories, y=counts)
plt.title('Counts of Fraud Transactions by Category')
plt.xlabel('Category')
plt.ylabel('Count')
plt.xticks(rotation=45)

final_data = data_clean.loc[:, ['amount', 'type_CASH_OUT', 'type_TRANSFER', 'recipient_C', 'oldBalOrig Flag', 'newBalOrig Flag', 'oldBalDest Flag','newBalDest Flag','isFraud']]

"""# **Predictive Modeling**

**Split Dataset**

Split data into training and testing sets, then take 20% of the training set to be used as a validation set.
"""

features = final_data.drop(['isFraud'], axis = 1)
target = final_data['isFraud']

# split data into testing and training
x_train, x_test, y_train, y_test = train_test_split(features, target, train_size = 0.8, test_size = 0.2, stratify = target)
# slit training set into training and validation
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size = 0.8, test_size = 0.2, stratify = y_train)

"""Outcome is either 1 - fraudulent or 0 - non-fraudulent, thus we use binary classification.

We want to use a neural network for this because:

*   Low correlation indicates poor linear relationship so using a neural netwoork will be better in capturing complex patterns
"""

from tensorflow.python.keras import models
from tensorflow.python.keras import layers
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

"""To efficiently identify the most optimal point, we create an algorithm that identifies different accuracies of the model when predicting the validation set.

Algorithm tests out different number of nodes per layer and number of epochs combination.

Manually have to add layers, so we list down the most optimal epoch and node combination for each layer.
"""

table = []

for nodes in range(7,20):

    for epochSize in range(10, 21):

        model = models.Sequential()

        model.add(layers.Dense(nodes, activation='relu', input_shape=(8,)))
        model.add(layers.Dense(nodes, activation='relu'))
        model.add(layers.Dense(nodes, activation='relu'))
        model.add(layers.Dense(nodes, activation='relu'))
        model.add(layers.Dense(nodes, activation='relu'))
        model.add(layers.Dense(nodes, activation='relu'))
        model.add(layers.Dense(nodes, activation='relu'))
        model.add(layers.Dense(nodes, activation='relu'))
        model.add(layers.Dense(nodes, activation='relu'))
        model.add(layers.Dense(nodes, activation='relu'))
        model.add(layers.Dense(nodes, activation='relu'))
        model.add(layers.Dense(1, activation='sigmoid'))

        model.compile(optimizer='adam',
                      loss='binary_crossentropy',
                      metrics=['accuracy'])

        model.fit(x_train, y_train, epochs = epochSize, batch_size = 60, verbose = 0)

        eval = model.evaluate(x_val, y_val)
        accuracy = eval[1]

        table.append([nodes, epochSize, accuracy])

df = pd.DataFrame(table, columns=['Neurons', 'Epoch Size', 'Accuracy'])

df[df['Accuracy'] == df['Accuracy'].max()]

"""*   Hidden Layers = 2, Nodes = 11 , Epoch = 12 : 72.8%
*   Hidden Layers = 3, Nodes = 10 , Epoch = 16 : 73.33%
*   Hidden Layers = 4, Nodes = 7, Epoch = 10 : 74.8%
*   Hidden Layers = 5, Nodes = 14, Epoch = 13 :  73.6%
*   Hidden Layers = 6, Nodes = 13 , Epoch =  17 : 73.78%
*   Hidden Layers = 7, Nodes = 8, Epoch = 17 : 73.74%
*   Hidden Layers = 8, Nodes = 9, Epoch = 16 : 73.74%
*   Hidden Layers = 9, Nodes = 9, Epoch = 19 : 73.8%
*   Hidden Layers = 10, Nodes = 17, Epoch = 15: 73.74%
*   Hidden Layers = 11, Nodes = 15, Epoch = 10 : 74.15%

The values are no longer improving, so we can select our most optimal parameter.

**Build Final Model Here**

Get the combination that gets you the highest accuracy and predict testing set, and run it again on the validation set.
"""

model = models.Sequential()

model.add(layers.Dense(7, activation='relu', input_shape=(8,)))
model.add(layers.Dense(7, activation='relu'))
model.add(layers.Dense(7, activation='relu'))
model.add(layers.Dense(7, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer = 'adam',
              loss = 'binary_crossentropy',
              metrics = ['accuracy'])

model.fit(x_train, y_train, epochs = 10, batch_size = 60, verbose = 0)

model.evaluate(x_val, y_val)

prediction = model.predict_classes(x_val)
accuracy = accuracy_score(y_val, prediction)
precision = precision_score(y_val, prediction)
recall = recall_score(y_val, prediction)
f1 = f1_score(y_val, prediction)

print('Accuracy:' , accuracy, '\nPrecision:', precision, '\nRecall:', recall, '\nF1:', f1)

"""**Evaluate model performance on testing set.**"""

model.evaluate(x_test, y_test)

predictionTest = model.predict_classes(x_test)
accuracyTest = accuracy_score(y_test, predictionTest)
precisionTest = precision_score(y_test, predictionTest)
recallTest = recall_score(y_test, predictionTest)
f1Test = f1_score(y_test, predictionTest)

print('Accuracy:' , accuracyTest, '\nPrecision:',  precisionTest, '\nRecall:', recallTest, '\nF1:', f1Test)

import matplotlib.pyplot as plt
import numpy as np

# Convert the DataFrame to a pivot table
pivot_table = df.pivot('Epoch Size', 'Neurons', 'Accuracy')

# Create a heatmap
plt.imshow(pivot_table, cmap='RdBu', interpolation='nearest')

# Set labels for the axes
plt.xlabel('Neurons')
plt.ylabel('Epoch Size')

# Add a colorbar for accuracy
cbar = plt.colorbar()
cbar.set_label('Average Accuracy')

# Show the plot
plt.show()

